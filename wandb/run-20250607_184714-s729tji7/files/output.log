Loading GSM8K data...
Train examples: 7473
Test examples: 1319
Loading Gemma-2B with LoRA...
Loading checkpoint shards: 100%|█████████████████████████████████| 2/2 [00:07<00:00,  3.63s/it]
trainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765
Tokenizing datasets...
Map: 100%|███████████████████████████████████████| 7473/7473 [00:00<00:00, 19609.23 examples/s]
Map: 100%|███████████████████████████████████████| 1319/1319 [00:00<00:00, 20397.11 examples/s]
Traceback (most recent call last):
  File "/Users/pablo/programming/cs/papers/01_lets_verify_step_by_step/train.py", line 125, in <module>
    trainer = Trainer(
  File "/Users/pablo/programming/cs/venv/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/Users/pablo/programming/cs/venv/lib/python3.9/site-packages/transformers/trainer.py", line 471, in __init__
    self.create_accelerator_and_postprocess()
  File "/Users/pablo/programming/cs/venv/lib/python3.9/site-packages/transformers/trainer.py", line 5176, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
  File "/Users/pablo/programming/cs/venv/lib/python3.9/site-packages/accelerate/accelerator.py", line 547, in __init__
    raise ValueError(f"fp16 mixed precision requires a GPU (not {self.device.type!r}).")
ValueError: fp16 mixed precision requires a GPU (not 'mps').
