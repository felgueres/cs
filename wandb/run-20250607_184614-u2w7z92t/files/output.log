Loading GSM8K data...
Train examples: 7473
Test examples: 1319
Loading Gemma-2B with LoRA...
Loading checkpoint shards: 100%|█████████████████████████████████| 2/2 [00:08<00:00,  4.02s/it]
trainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765
Tokenizing datasets...
Map: 100%|███████████████████████████████████████| 7473/7473 [00:00<00:00, 16719.09 examples/s]
Map: 100%|███████████████████████████████████████| 1319/1319 [00:00<00:00, 18325.63 examples/s]
Traceback (most recent call last):
  File "/Users/pablo/programming/cs/papers/01_lets_verify_step_by_step/train.py", line 103, in <module>
    training_args = TrainingArguments(
  File "<string>", line 131, in __init__
  File "/Users/pablo/programming/cs/venv/lib/python3.9/site-packages/transformers/training_args.py", line 1649, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the saving steps to be a round multiple of the evaluation steps, but found 500, which is not a round multiple of 200.
